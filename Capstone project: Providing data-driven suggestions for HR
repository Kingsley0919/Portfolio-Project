Step 1. Imports
Import packages
Load dataset
# Import packages
import pandas as pd
import numpy as np
# Import packages for data visualization
import matplotlib.pyplot as plt
import seaborn as sns


# Import packages for data visualization
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import roc_auc_score, roc_curve, auc
from sklearn.metrics import accuracy_score, precision_score, recall_score,\
f1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay
from sklearn.utils import resample
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# This is the function that helps plot feature importance
from xgboost import plot_importance


# This module lets us save our models once we fit them.
import pickle

# Import packages
import pandas as pd
import numpy as np
# Import packages for data visualization
import matplotlib.pyplot as plt
import seaborn as sns
â€‹
â€‹
# Import packages for data visualization
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import roc_auc_score, roc_curve, auc
from sklearn.metrics import accuracy_score, precision_score, recall_score,\
f1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay
from sklearn.utils import resample
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
â€‹
# This is the function that helps plot feature importance
from xgboost import plot_importance
â€‹
â€‹
# This module lets us save our models once we fit them.
import pickle
â€‹
â€‹
# Load dataset into a dataframe
â€‹
df0 = pd.read_csv(r"C:\Users\User\Desktop\codecademy\Activity_ Course 7 Salifort Motors project lab\work\HR_capstone_dataset.csv")
â€‹
â€‹
# Display first few rows of the dataframe
df0.head(20)
â€‹
Step 2. Data Exploration (Initial EDA and data cleaning)
Understand your variables
Clean your dataset (missing data, redundant data, outliers)
# Gather basic information about the data
df0.info()
# Gather descriptive statistics about the data
df0.describe()
###EDA
â€‹
# Rename columns
'''As a data cleaning step, rename the columns as needed. Standardize the column names so that they are all in `snake_case`, 
correct any column names that are misspelled, and make column names more concise as needed.'''
â€‹
# Display all column names
df0.columns
â€‹
# Rename columns as needed
df0.rename(columns={"Work_accident":"work_accident","Department":"department"},inplace=True)
â€‹
# Display all column names after the update
df0.columns
â€‹
# Check for missing values
df0.isnull().sum()
â€‹
# Check for duplicates
duplicate_counts = df0.apply(lambda x: x.duplicated().sum())
print(duplicate_counts)
â€‹
# Inspect some rows containing duplicates as needed
df0.loc[df0.duplicated()]
â€‹
# Drop duplicates and save resulting dataframe in a new variable as needed
df = df0.drop_duplicates()
â€‹
# Display first few rows of new dataframe as needed
df.info()
â€‹
â€‹
df_outlier = df.iloc[:,0:5]
â€‹
# Determine the number of rows containing outliers for each column
for column in df_outlier:
    data = df_outlier[column]
â€‹
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1
â€‹
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
â€‹
    outliers = data[(data < lower_bound) | (data > upper_bound)]
â€‹
    print(f"Column: {column}")
    print(f"IQR: {iqr}")
    print(f"Lower Bound: {lower_bound}")
    print(f"Upper Bound: {upper_bound}")
    print(f"Outliers: {outliers}\n")
â€‹
    num_outliers = len(outliers)
    print(f"Number of outliers in column {column}: {num_outliers}\n")
â€‹
Step 2. Data Exploration (Continue EDA)
Begin by understanding how many employees left and what percentage of all employees this figure represents.

# Get numbers of people who left vs. stayed
df['left'].value_counts()
â€‹
# Get percentages of people who left vs. stayed
â€‹
df['left'].value_counts(normalize = True)
â€‹
X
# Create a plot as needed
df_numeric = df[["satisfaction_level","last_evaluation","number_project","average_montly_hours","time_spend_company"]]
df_category = df[["work_accident","promotion_last_5years","department","salary"]]
label = df["left"]
â€‹
# Box plots for numerical features
for feature in df_numeric.columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=label, y=feature, data=df)
    plt.xlabel('left')
    plt.ylabel(feature)
    plt.title(f'Box Plot of {feature} by left')
    plt.show()
# Count plots for categorical features
for feature in df_category.columns:
    plt.figure(figsize=(8, 6))
    sns.countplot(x=feature, hue=label, data=df, palette='pastel')
    plt.xlabel(feature)
    plt.ylabel('Count')
    plt.title(f'Count Plot of {feature} with respect to left')
    plt.legend(title='left', loc='upper right', labels=['Not Left', 'Left'])
    plt.show()
    
    
sns.heatmap(df_numeric, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.show()
ðŸ”Ž

Recall model assumptions
Logistic Regression model assumptions

Outcome variable is categorical
Observations are independent of each other
No severe multicollinearity among X variables
No extreme outliers
Linear relationship between each X variable and the logit of the outcome variable
Sufficiently large sample size
#hypothesis testing
# Perform correlation analysis for each numerical feature
from scipy.stats import pearsonr
from scipy.stats import chi2_contingency
from sklearn.preprocessing import OneHotEncoder
â€‹
for feature in df_numeric.columns:
    
    correlation, p_value = pearsonr(df[feature], df['left'])
    print(f"{feature} - Correlation: {correlation}, p-value: {p_value}")
    
    # Perform chi-square test for each categorical feature
for feature in df_category.columns:
    contingency_table = pd.crosstab(df[feature], df['left'])
    chi2, p_value, dof, expected = chi2_contingency(contingency_table)
    print(f"{feature} - Chi-Square: {chi2}, p-value: {p_value}")
    
â€‹
    
#Perform resample
â€‹
df_majority = df[df['left'] == 0]
df_minority = df[df['left'] == 1]
# Upsample the minority class
df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)
â€‹
# Combine the majority class and upsampled minority class
df_upsampled = pd.concat([df_majority, df_minority_upsampled])
â€‹
# Check the new class distribution
print(df_upsampled['left'].value_counts())
â€‹
â€‹
â€‹
# Assuming you have df_upsampled as the original DataFrame
â€‹
# Replace 'NaN' with NaN (actual missing value representation)
df_upsampled.replace('NaN', pd.NA, inplace=True)
â€‹
# Extract the 'department' and 'salary' columns
departments = df_upsampled[['department']]
salaries = df_upsampled[['salary']]
â€‹
# Create an instance of OneHotEncoder for 'department' column
department_encoder = OneHotEncoder()
â€‹
# Fit the encoder on the 'department' column
department_encoder.fit(departments)
â€‹
# Transform the 'department' column
departments_encoded = department_encoder.transform(departments)
â€‹
# Convert the encoded array back to a DataFrame
departments_encoded_df = pd.DataFrame(departments_encoded.toarray(), columns=department_encoder.get_feature_names(['department']))
â€‹
# Create an instance of OneHotEncoder for 'salary' column
salary_encoder = OneHotEncoder()
â€‹
# Fit and transform the 'salary' column
salaries_encoded = salary_encoder.fit_transform(salaries)
â€‹
# Convert the encoded array back to a DataFrame
salaries_encoded_df = pd.DataFrame(salaries_encoded.toarray(), columns=salary_encoder.get_feature_names(['salary']))
â€‹
# Reset the index of the DataFrames
departments_encoded_df.reset_index(drop=True, inplace=True)
salaries_encoded_df.reset_index(drop=True, inplace=True)
â€‹
# Drop the original 'department' and 'salary' columns
df_upsampled.drop(['department', 'salary'], axis=1, inplace=True)
â€‹
# Concatenate the encoded DataFrames with the original DataFrame
df_upsampled_encoded = pd.concat([df_upsampled.reset_index(drop=True), departments_encoded_df, salaries_encoded_df], axis=1)
â€‹
# Display the merged DataFrame
df_upsampled_encoded.head()
â€‹
satisfaction_level:

Correlation: -0.3506 p-value: 0.0000 Conclusion: The satisfaction level is negatively correlated with the target variable (left). A lower satisfaction level is associated with a higher likelihood of employees leaving the company.

last_evaluation:

Correlation: 0.0135 p-value: 0.1388 Conclusion: The last evaluation does not show a statistically significant correlation with the target variable (left) as the p-value is greater than the typical significance level of 0.05.

number_project:

Correlation: 0.0309 p-value: 0.0007 Conclusion: The number of projects an employee has been involved in has a statistically significant, but weak positive correlation with the target variable (left).

average_montly_hours:

Correlation: 0.0704 p-value: 1.1739e-14 Conclusion: The average monthly hours worked by an employee has a statistically significant, but relatively weak positive correlation with the target variable (left).

time_spend_company:

Correlation: 0.1733 p-value: 1.7458e-81 Conclusion: The time spent by an employee in the company has a statistically significant positive correlation with the target variable (left). Employees who have spent more time in the company are more likely to leave.

work_accident:

Chi-Square: 187.7378 p-value: 9.9112e-43 Conclusion: The occurrence of a work accident is associated with a statistically significant relationship with the target variable (left). Employees who had a work accident are more likely to leave the company.

promotion_last_5years:

Chi-Square: 22.9921 p-value: 1.6267e-06 Conclusion: The promotion status of employees in the last 5 years has a statistically significant relationship with the target variable (left). Employees who received promotions in the last 5 years are less likely to leave the company.

department:

Chi-Square: 20.8575 p-value: 0.0133 Conclusion: The department in which an employee works has a statistically significant relationship with the target variable (left). The department variable may have some influence on employee churn.

salary:

Chi-Square: 175.2107 p-value: 8.9841e-39 Conclusion: Salary level is associated with a statistically significant relationship with the target variable (left). Employees with different salary levels may have different chances of leaving the company.

In conclusion, based on the provided hypothesis testing results, satisfaction level, number of projects, average monthly hours, time spent in the company, work accident, promotion status, department, and salary level are all statistically significant factors related to the likelihood of employees leaving the company (left). The last evaluation, while showing a positive correlation, does not demonstrate statistical significance in predicting employee churn.

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
â€‹
X = df[['satisfaction_level', 'number_project', 'average_montly_hours', 'time_spend_company', 'work_accident', 'promotion_last_5years']]
y = df['left']
â€‹
# Calculate VIF for each predictor
vif = pd.DataFrame()
vif["Features"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif)
â€‹
Step 3. Model Building, Step 4. Results and Evaluation
Fit a model that predicts the outcome variable using two or more independent variables
Check model assumptions
Evaluate the model
# 1.Drop the columns 'left' and 'last_evaluation' from the DataFrame 'df'
X = df_upsampled_encoded.drop(['left', 'last_evaluation','average_montly_hours'], axis=1)
â€‹
# 2. Create the target variable y using the 'left' column from the DataFrame 'df_upsampled'
y = df_upsampled_encoded['left']
â€‹
# 3. Split into train and test sets
X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, stratify=y,
                                              test_size=0.2, random_state=42)
â€‹
# 4. Split into train and validate sets
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, stratify=y_trainval,
                                                  test_size=0.25, random_state=42)
RandomForestClassifier
# 1. Instantiate the random forest classifier
rf = RandomForestClassifier(random_state=42)
â€‹
# 2. Create a dictionary of hyperparameters to tune
cv_params = {'max_depth': [None],
             'max_features': [1.0],
             'max_samples': [1.0],
             'min_samples_leaf': [2],
             'min_samples_split': [2],
             'n_estimators': [300],
             }
â€‹
# 3. Define a dictionary of scoring metrics to capture
scoring = {'accuracy', 'precision', 'recall', 'f1'}
â€‹
# 4. Instantiate the GridSearchCV object
rf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='recall')
â€‹
%%time
# Fit the GridSearchCV to the data
â€‹
rf_cv.fit(X_train, y_train)
# Examine best score
rf_cv.best_score_
# Examine best hyperparameter combo
rf_cv.best_params_
def make_results(model_name:str, model_object, metric:str):
    '''
    Arguments:
        model_name (string): what you want the model to be called in the output table
        model_object: a fit GridSearchCV object
        metric (string): precision, recall, f1, or accuracy
â€‹
    Returns a pandas df with the F1, recall, precision, and accuracy scores
    for the model with the best mean 'metric' score across all validation folds.
    '''
â€‹
    # Create dictionary that maps input metric to actual metric name in GridSearchCV
    metric_dict = {'precision': 'mean_test_precision',
                   'recall': 'mean_test_recall',
                   'f1': 'mean_test_f1',
                   'accuracy': 'mean_test_accuracy',
                   }
â€‹
    # Get all the results from the CV and put them in a df
    cv_results = pd.DataFrame(model_object.cv_results_)
â€‹
    # Isolate the row of the df with the max(metric) score
    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]
â€‹
    # Extract accuracy, precision, recall, and f1 score from that row
    f1 = best_estimator_results.mean_test_f1
    recall = best_estimator_results.mean_test_recall
    precision = best_estimator_results.mean_test_precision
    accuracy = best_estimator_results.mean_test_accuracy
â€‹
    # Create table of results
    table = pd.DataFrame({'model': [model_name],
                          'precision': [precision],
                          'recall': [recall],
                          'F1': [f1],
                          'accuracy': [accuracy],
                          },
                         )
â€‹
    return table
def get_test_scores(model_name:str, preds, y_test_data):
    '''
    Generate a table of test scores.
â€‹
    In:
        model_name (string): Your choice: how the model will be named in the output table
        preds: numpy array of test predictions
        y_test_data: numpy array of y_test data
â€‹
    Out:
        table: a pandas df of precision, recall, f1, and accuracy scores for your model
    '''
    accuracy = accuracy_score(y_test_data, preds)
    precision = precision_score(y_test_data, preds)
    recall = recall_score(y_test_data, preds)
    f1 = f1_score(y_test_data, preds)
â€‹
    table = pd.DataFrame({'model': [model_name],
                          'precision': [precision],
                          'recall': [recall],
                          'F1': [f1],
                          'accuracy': [accuracy]
                          })
â€‹
    return table
results = make_results('RF cv', rf_cv, 'recall')
results
# Use random forest model to predict on validation data
rf_val_preds = rf_cv.best_estimator_.predict(X_val)
# Get validation scores for RF model
rf_val_scores = get_test_scores('RF val', rf_val_preds, y_val)
â€‹
# Append to the results table
results = pd.concat([results, rf_val_scores], axis=0)
results
XGBClassifier
# 1. Instantiate the XGBoost classifier
xgb = XGBClassifier(objective='binary:logistic', random_state=42)
â€‹
# 2. Create a dictionary of hyperparameters to tune
cv_params = {'max_depth': [6, 12],
             'min_child_weight': [3, 5],
             'learning_rate': [0.01, 0.1],
             'n_estimators': [300]
             }
â€‹
# 3. Define a dictionary of scoring metrics to capture
scoring = {'accuracy', 'precision', 'recall', 'f1'}
â€‹
# 4. Instantiate the GridSearchCV object
xgb_cv = GridSearchCV(xgb, cv_params, scoring=scoring, cv=4, refit='recall')
â€‹
xgb_cv.fit(X_train, y_train)
â€‹
# Examine best score
xgb_cv.best_score_
â€‹
# Examine best parameters
xgb_cv.best_params_
â€‹
â€‹
# Call 'make_results()' on the GridSearch object
xgb_cv_results = make_results('XGB cv', xgb_cv, 'recall')
results = pd.concat([results, xgb_cv_results], axis=0)
results
â€‹
# Use XGBoost model to predict on validation data
xgb_val_preds = xgb_cv.best_estimator_.predict(X_val)

# Get validation scores for XGBoost model
xgb_val_scores = get_test_scores('XGB val', xgb_val_preds, y_val)

# Append to the results table
results = pd.concat([results, xgb_val_scores], axis=0)
results
# Use XGBoost model to predict on validation data
xgb_val_preds = xgb_cv.best_estimator_.predict(X_val)
â€‹
# Get validation scores for XGBoost model
xgb_val_scores = get_test_scores('XGB val', xgb_val_preds, y_val)
â€‹
# Append to the results table
results = pd.concat([results, xgb_val_scores], axis=0)
results
# 1. Instantiate the Logistic Regression model
lr = LogisticRegression(random_state=42)
â€‹
# 2. Create a dictionary of hyperparameters to tune
param_grid = {
    'penalty': ['l2'],
    'C': [0.01, 0.1, 1, 10],
    'solver': ['liblinear', 'lbfgs']
}
â€‹
# 3. Define a dictionary of scoring metrics to capture
scoring = {'accuracy', 'precision', 'recall', 'f1'}
â€‹
# 4. Instantiate the GridSearchCV object for Logistic Regression
lr_cv = GridSearchCV(lr, lr_params, scoring=scoring, cv=4, refit='recall')
â€‹
# Fit the GridSearchCV to the data
lr_cv.fit(X_train, y_train)
â€‹
# Examine best score
lr_cv.best_score_
â€‹
# Examine best parameters
lr_cv.best_params_
â€‹
# Call 'make_results()' on the GridSearch object for Logistic Regression
lr_cv_results = make_results('Logistic Regression cv', lr_cv, 'recall')
â€‹
# Concatenate the results with the existing results table
results = pd.concat([results, lr_cv_results], axis=0)
â€‹
# Print the updated results table
print(results)
â€‹
# Use logistic model to predict on validation data
lr_val_preds = lr_cv.best_estimator_.predict(X_val)
â€‹
# Get validation scores for XGBoost model
lr_val_scores = get_test_scores('Logistic Regression val', lr_val_preds,  y_val)
â€‹
# Append to the results table
results = pd.concat([results, lr_val_scores], axis=0)
results
# Use XGBoost model to predict on test data
xgb_test_preds = xgb_cv.best_estimator_.predict(X_test)
â€‹
# Get test scores for XGBoost model
xgb_test_scores = get_test_scores('XGB test', xgb_test_preds, y_test)
â€‹
# Append to the results table
results = pd.concat([results, xgb_test_scores], axis=0)
results
left
# Generate array of values for confusion matrix
cm = confusion_matrix(y_test, xgb_test_preds, labels=xgb_cv.classes_)
â€‹
# Plot confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                             display_labels=['retained', 'left'])
disp.plot();
