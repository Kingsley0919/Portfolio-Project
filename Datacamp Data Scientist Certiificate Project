
import pandas as pd
import numpy as np
#Import the dataset

df = pd.read_csv("recipe_site_traffic_2212.csv")

#Inspect the first ten rows of the data
df.head(10)
​
#Shape of the data set
df.shape

#Inspect the data type and missing value of the data
df.info()

#Basic statistic of the data
df.describe()
​
#Check for duplicate row
df.duplicated().sum()
​
#Check for not tally value in the serving column
df['servings'].value_counts(normalize = True)
​
#Check for not all category in category column
df['category'].value_counts(normalize = True)


<class 'pandas.core.frame.DataFrame'>
RangeIndex: 947 entries, 0 to 946
Data columns (total 8 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   recipe        947 non-null    int64  
 1   calories      895 non-null    float64
 2   carbohydrate  895 non-null    float64
 3   sugar         895 non-null    float64
 4   protein       895 non-null    float64
 5   category      947 non-null    object 
 6   servings      947 non-null    object 
 7   high_traffic  574 non-null    object 
dtypes: float64(4), int64(1), object(3)
memory usage: 59.3+ KB
Breakfast         0.111932
Chicken Breast    0.103485
Beverages         0.097149
Lunch/Snacks      0.093981
Potato            0.092925
Pork              0.088701
Vegetable         0.087645
Dessert           0.087645
Meat              0.083421
Chicken           0.078141
One Dish Meal     0.074974
Name: category, dtype: float64

Data Validation:
This dataset has 947 rows, 8 columns. I have validate all variables and I have not made any changes after validation. All the columns are just as described in the data dictionary and there is no duplicated row. There are some abnormal value exist in calories column.

recipe:
All row has distinct and numeric value, no missing value exist, therefore no cleaning is needed.

calories:
52 missing value exist in calories column, all value are numeric, cleaning is needed.

carbohydrate:
52 missing value exist in carbohydrate column, all value are numeric, cleaning is needed.

sugar:
52 missing value exist in sugar column, all value are numeric, cleaning is needed.

protein:
52 missing value exist in protein column, all value are numeric, cleaning is needed.

category:
No missing value exist in category column, there are total 11 categories, one extra category from the description is "Chicken Breast", cleaning is needed.

servings:
No missing value exist in serving column, however there are different type of value exist in this column, some row contain only one numeric value, while other contain some description, cleaning is needed.

high_traffic:
373 missing value, however missing value has it meaning, we should encode it and one one type of value exist which is "high", cleaning is needed




#Drop the missing value column in 'calories', 'carbohydrate', 'sugar', 'protein' columns
columns_to_dropna = ['calories', 'carbohydrate', 'sugar', 'protein']
df1 = df.dropna(subset = columns_to_dropna)
df1.info()

#Standardize the value in category column
df1["category"].replace("Chicken Breast","Chicken",inplace = True)
df1["category"].value_counts()


#Standardize the valu in servings column
value_to_replace = {"4 as a snack": "4","6 as a snack":"6"}
df1["servings"].replace(value_to_replace, inplace = True)
df1["servings"].value_counts()
df1["servings"] = df1["servings"].astype("int64")
​

#Encode the target variable
df1["high_traffic"].fillna(0,inplace = True)
df1["high_traffic"].replace("High",1,inplace = True)
df1.info()
​
<class 'pandas.core.frame.DataFrame'>
Int64Index: 895 entries, 1 to 946
Data columns (total 8 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   recipe        895 non-null    int64  
 1   calories      895 non-null    float64
 2   carbohydrate  895 non-null    float64
 3   sugar         895 non-null    float64
 4   protein       895 non-null    float64
 5   category      895 non-null    object 
 6   servings      895 non-null    object 
 7   high_traffic  535 non-null    object 
dtypes: float64(4), int64(1), object(3)
memory usage: 62.9+ KB
<class 'pandas.core.frame.DataFrame'>
Int64Index: 895 entries, 1 to 946
Data columns (total 8 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   recipe        895 non-null    int64  
 1   calories      895 non-null    float64
 2   carbohydrate  895 non-null    float64
 3   sugar         895 non-null    float64
 4   protein       895 non-null    float64
 5   category      895 non-null    object 
 6   servings      895 non-null    int64  
 7   high_traffic  895 non-null    int64  
dtypes: float64(4), int64(3), object(1)
memory usage: 62.9+ KB

Exploratory Analysis:
I have investigated the target variable and features of the the receipt, and the relationship between target variable and features. After the analysis,I decided to apply the following changes to enable modeling:

Remove missing value in column calories,carbohydrate,sugar,protein column.
Since column category "chicken breast" are the same as "chicken", we will replace this category with "chicken".
Standardize the type of value and transform them to numeric type in servings column
Replace null value in column high_traffic as 0, and "high" as 1

import matplotlib.pyplot as plt 
import seaborn as sns 
​
numeric_column = ['calories', 'carbohydrate', 'sugar', 'protein']
category_column = ['category', 'servings']

# Set up the subplots
fig, axes = plt.subplots(nrows=2, ncols=len(numeric_column), figsize=(16, 8))
fig.suptitle('Boxplots and Histograms of Nutritional Information', y=1.02, fontsize=16)

# Create boxplots and histograms
for idx, col in enumerate(numeric_column):
    sns.boxplot(data=df, x=col, ax=axes[0, idx])
    axes[0, idx].set_xticks([])
    sns.histplot(data=df, x=col, ax=axes[1, idx], kde=True)
    axes[1, idx].set_xlabel(col)

# Set x-axis labels for the last row of subplots
for ax in axes[1]:
    ax.set_xlabel('Value')
​
# Remove y-axis labels from histograms in the first row
for ax in axes[0]:
    ax.set_ylabel('')
    ax.set_yticklabels([])

plt.tight_layout()
plt.show()

The combination of log-normal distribution and the presence of outliers suggests that this dataset contains data with a wide range of values, including some extreme values that may be statistically significant or influential. we might decide to address outliers through techniques such as transformation, winsorization, or removing them if they are erroneous.After that i decided to replace the outliers withe it median and use scale the value for each numeric column.


# Set style and context
sns.set(style="ticks", context="talk")

# Create the pair plot
pair_plot = sns.pairplot(df[numeric_column], diag_kind='kde', markers='o')

# Customize the plot labels and title
pair_plot.fig.suptitle("Pair Plot of Nutritional Information", y=1.02)
plt.tight_layout()
plt.show()

The pair plot above does not reveal any strong relationships between the columns. To further explore potential correlations, we will utilize a heatmap in our analysis. This heatmap will provide a more comprehensive visualization of the relationships between the nutritional attributes.

from sklearn.preprocessing import StandardScaler

# Columns to be standardized
columns_to_standardize = ['calories', 'carbohydrate', 'sugar', 'protein']

# Create a StandardScaler instance
scaler = StandardScaler()

# Apply the scaler to selected columns
df1[columns_to_standardize] = scaler.fit_transform(df1[columns_to_standardize])


# Calculate the IQR for each column
Q1 = df1.quantile(0.25)
Q3 = df1.quantile(0.75)
IQR = Q3 - Q1

# Determine bounds for identifying outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Replace outliers with the median value
for column in ['calories', 'carbohydrate', 'sugar', 'protein']:
    df1[column] = df1[column].apply(lambda x: x if (lower_bound[column] <= x <= upper_bound[column]) else df1[column].median())

print(df1)

from sklearn.preprocessing import StandardScaler

# Columns to be standardized
columns_to_standardize = ['calories', 'carbohydrate', 'sugar', 'protein']
​
# Create a StandardScaler instance
scaler = StandardScaler()
​
# Apply the scaler to selected columns
df1[columns_to_standardize] = scaler.fit_transform(df1[columns_to_standardize])
​

# Calculate the IQR for each column
Q1 = df1.quantile(0.25)
Q3 = df1.quantile(0.75)
IQR = Q3 - Q1
​
# Determine bounds for identifying outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
​

# Replace outliers with the median value
for column in ['calories', 'carbohydrate', 'sugar', 'protein']:
    df1[column] = df1[column].apply(lambda x: x if (lower_bound[column] <= x <= upper_bound[column]) else df1[column].median())

print(df1)
     recipe  calories  carbohydrate  ...       category  servings high_traffic
1         2 -0.884469      0.079462  ...         Potato         4            1
2         3  1.056481      0.173259  ...      Breakfast         1            0
3         4 -0.748528     -0.102669  ...      Beverages         4            1
4         5 -0.903088     -0.756291  ...      Beverages         4            0
5         6  0.563668     -0.719637  ...  One Dish Meal         2            1
..      ...       ...           ...  ...            ...       ...          ...
941     942 -0.551561      1.112599  ...        Chicken         4            1
942     943  1.601396     -0.677519  ...   Lunch/Snacks         2            0
944     945  1.139217     -0.128623  ...           Pork         2            1
945     946 -0.373987      0.015944  ...         Potato         6            1
946     947 -0.555205      0.230858  ...      Beverages         4            0

[895 rows x 8 columns]
Standardization, done using the StandardScaler from the sklearn.preprocessing module, makes sure that all the numerical attributes are on the same scale. This is important for algorithms like Random Forest and XGBoost, which can be sensitive to varying scales of features. By applying the scaler to columns like 'calories', 'carbohydrate', 'sugar', and 'protein', we're making sure that these features are comparable in size, which helps our models work better.

We've also taken care of outliers. Outliers are extreme values that can mess up our models. To handle them, we calculated the interquartile range (IQR) for each column and figured out the bounds for identifying outliers. Then, we replaced these outliers with the median value. This way, we're making sure that any unusually large or small values don't mess up our models.

These steps might sound technical, but they're all about making our data clean, consistent, and ready for our Random Forest and XGBoost classifiers. By doing this, we're giving our models the best chance to make accurate predictions based on standardized and well-handled features.


df1.corr()
corr = df1.corr()

# Create a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

The correlation values indicate that there aren't strong linear relationships between the variables in the given dataset. Keep in mind that correlation only measures linear relationships and does not capture potential nonlinear dependencies or other complex interactions between variables. Additionally, it's important to interpret these correlations in the context of the domain and the specific data being analyzed.

import matplotlib.pyplot as plt
import seaborn as sns
​
# Create a 2x2 subplot grid
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
​
# Adjust the x-axis ticks, rotation, and size for the 'category' count plot
sns.countplot(data=df1, x='category', ax=axes[0, 0])
axes[0, 0].set_title('Count Plot of Category')
axes[0, 0].set_xlabel('Category')
axes[0, 0].set_ylabel('Count')
axes[0, 0].tick_params(axis='x', rotation=45, labelsize=8)  # Adjust x-axis ticks

# Adjust the x-axis ticks, rotation, and size for the 'servings' count plot
sns.countplot(data=df1, x='servings', ax=axes[0, 1])
axes[0, 1].set_title('Count Plot of Servings')
axes[0, 1].set_xlabel('Servings')
axes[0, 1].set_ylabel('Count')
axes[0, 1].tick_params(axis='x', rotation=45, labelsize=8)  # Adjust x-axis ticks

​

# Adjust the x-axis ticks, rotation, and size for the 'category' count plot with 'high_traffic' hue
sns.countplot(data=df1, x='category', hue='high_traffic', ax=axes[1, 0])
axes[1, 0].set_title('Count Plot of Category with High Traffic Hue')
axes[1, 0].set_xlabel('Category')
axes[1, 0].set_ylabel('Count')
axes[1, 0].legend(title='High Traffic', loc='upper right')
axes[1, 0].tick_params(axis='x', rotation=45, labelsize=8)  # Adjust x-axis ticks
​

# Adjust the x-axis ticks, rotation, and size for the 'servings' count plot with 'high_traffic' hue
sns.countplot(data=df1, x='servings', hue='high_traffic', ax=axes[1, 1])
axes[1, 1].set_title('Count Plot of Servings with High Traffic Hue')
axes[1, 1].set_xlabel('Servings')
axes[1, 1].set_ylabel('Count')
axes[1, 1].legend(title='High Traffic', loc='upper right')
axes[1, 1].tick_params(axis='x', rotation=45, labelsize=8)  # Adjust x-axis ticks

# Adjust spacing between subplots
plt.tight_layout()
plt.show()
​


# Pivot the data using pivot_table
pivot_data_category = df1.pivot_table(index='category', columns='high_traffic', aggfunc='size', fill_value=0)
pivot_data_servings = df1.pivot_table(index = 'servings', columns = 'high_traffic', aggfunc = 'size',fill_value=0)
print(pivot_data_category)
print(pivot_data_servings)


high_traffic    0   1
category             
Beverages      87   5
Breakfast      73  33
Chicken        94  69
Dessert        29  48
Lunch/Snacks   30  52
Meat           18  56
One Dish Meal  16  51
Pork            7  66
Potato          5  78
Vegetable       1  77
high_traffic    0    1
servings              
1              70   99
2              76   98
4             147  220
6              67  118
Based on the provided count plots for different values of high_traffic, we can draw the following conclusions:

Category vs. High Traffic:
When considering different recipe categories, the countplot clearly indicates varying patterns of high traffic. For instance:

Beverages, Breakfast, Chicken, and Pork: These categories exhibit a relatively balanced distribution between high and low traffic. Breakfast and Chicken recipes seem to have a higher propensity for attracting high traffic, while Beverages and Pork recipes tend to have lower proportions of high traffic. Dessert and Lunch/Snacks: The Dessert and Lunch/Snacks categories display a more noticeable skew towards high traffic, indicating that these types of recipes might be more engaging for users. Meat, One Dish Meal, Potato, and Vegetable: These categories lean towards high traffic as well, with One Dish Meal and Vegetable recipes showing particularly strong engagement.

Servings vs. High Traffic:
The second countplot explores the relationship between the number of servings in a recipe and its likelihood to generate high traffic:

Across the board, recipes with 4 servings dominate both high and low traffic scenarios, suggesting that this serving size attracts a substantial audience. As the serving size increases to 6, recipes exhibit a higher likelihood of attracting high traffic, indicating that recipes designed to serve more people tend to be more popular. Notably, recipes with 1 and 2 servings lean more towards high traffic. These could potentially be single-serving meals or snacks that users find appealing and easy to try.

In summary, these countplots provide valuable insights into the factors influencing high website traffic. Categories like Breakfast, Chicken, Dessert, and Lunch/Snacks, as well as recipes with 4 and 6 servings, tend to draw higher traffic. This information is crucial for building a machine learning model that predicts recipe popularity, as it highlights the types of recipes that have a higher chance of attracting user engagement and subscriptions.

Model Fitting & Evaluation:
A binary classification model would be appropriate for this task, as it involves predicting whether a recipe will result in high traffic (1) or not (0). The model i choose are XGBoost model and Random Forest machine learning model, and i will use GridSearchCV to search for the best estimator and cross validate the dateset to find the best model for testing purpose.


from sklearn.model_selection import train_test_split
# One-hot encode the 'category' column
df_encoded = pd.get_dummies(df1, columns=['category'], drop_first=True)

df_encoded = df_encoded.drop(columns=['recipe'])

# Separate features (X) and target (y)
X = df_encoded.drop(columns=['high_traffic']) 
y = df_encoded['high_traffic']
​
# Splitting the data into train and test sets (30% test)
X_trn, X_test, y_trn, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
​
# Split the training data into training and validation sets(30% validate)
X_train, X_val, y_train, y_val = train_test_split(X_trn, y_trn, test_size=0.3, random_state=0)
Random Forest Classifier

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

​

# Instantiate the random forest classifier

rf = RandomForestClassifier(random_state=0)

​

# Create a dictionary of hyperparameters to tune

cv_params = {'max_depth': [5, 7, None],

             'max_features': [0.3, 0.6],

             'max_samples': [0.7],

             'min_samples_leaf': [1,2],

             'min_samples_split': [2,3],

             'n_estimators': [75,100,200],

             }

​
# Define a dictionary of scoring metrics to capture
scoring = {'accuracy', 'precision', 'recall', 'f1'}
​

# Instantiate the GridSearchCV object
rf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=5, refit='recall')

rf_cv.fit(X_train, y_train)
​
# Examine best recall score
rf_cv.best_score_
​
rf_cv.best_params_
{'max_depth': 5,
 'max_features': 0.3,
 'max_samples': 0.7,
 'min_samples_leaf': 1,
 'min_samples_split': 3,
 'n_estimators': 75}
XGBoost Classifier
xgb_cv.best_params_
from xgboost import XGBClassifier
# Instantiate the XGBoost classifier
xgb = XGBClassifier(objective='binary:logistic', random_state=0)

​

# Create a dictionary of hyperparameters to tune

cv_params = {'max_depth': [4,8,12],

             'min_child_weight': [3, 5],

             'learning_rate': [0.01, 0.1],

             'n_estimators': [300, 500]

             }

​

# Define a dictionary of scoring metrics to capture
scoring = {'accuracy', 'precision', 'recall', 'f1'}

​

# Instantiate the GridSearchCV object
xgb_cv = GridSearchCV(xgb, cv_params, scoring=scoring, cv=5, refit='recall')
xgb_cv.fit(X_train, y_train)
xgb_cv.best_score_

xgb_cv.best_params_
{'learning_rate': 0.01,
 'max_depth': 4,
 'min_child_weight': 3,
 'n_estimators': 300}
Random Forest Evaluation

from sklearn.metrics import classification_report, accuracy_score, precision_score, \
recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
​
# Use the random forest "best estimator" model to get predictions on the validation set
y_pred_rf = rf_cv.best_estimator_.predict(X_val)

​

# Create a confusion matrix to visualize the results of the classification model


# Compute values for confusion matrix
rf_cm = confusion_matrix(y_val, y_pred_rf)

​

# Create display of confusion matrix

rf_disp = ConfusionMatrixDisplay(confusion_matrix=rf_cm, display_labels=None)


# Plot confusion matrix
rf_disp.plot()

​
# Display plot
plt.title('RandomForest - validation set');
plt.show()


# Create a classification report
# Create classification report for random forest model
target_labels = ["Low_Traffic","High_Traffic"]

print(classification_report(y_val, y_pred_rf, target_names=target_labels))
              precision    recall  f1-score   support

 Low_Traffic       0.67      0.54      0.60        79
High_Traffic       0.71      0.81      0.76       109

    accuracy                           0.70       188
   macro avg       0.69      0.68      0.68       188
weighted avg       0.69      0.70      0.69       188

XGBOOST Evaluation

#Evaluate XGBoost model

y_pred_xgb = xgb_cv.best_estimator_.predict(X_val)

​

# Compute values for confusion matrix

xgb_cm = confusion_matrix(y_val, y_pred_xgb)

​

# Create display of confusion matrix

xgb_disp = ConfusionMatrixDisplay(confusion_matrix=xgb_cm, display_labels=None)

​

# Plot confusion matrix
xgb_disp.plot()


# Display plot
plt.title('XGBoost - validation set');
plt.show()

1
# Create a classification report
2
print(classification_report(y_val, y_pred_xgb, target_names=target_labels))
              precision    recall  f1-score   support

 Low_Traffic       0.64      0.59      0.62        79
High_Traffic       0.72      0.76      0.74       109

    accuracy                           0.69       188
   macro avg       0.68      0.68      0.68       188
weighted avg       0.69      0.69      0.69       188

Evaluation of The Final Model

# Deploy the higher score model

y_pred = rf_cv.best_estimator_.predict(X_test)


# Compute values for confusion matrix
rf_cm = confusion_matrix(y_test, y_pred)

# Create display of confusion matrix
rf_cm_disp = ConfusionMatrixDisplay(confusion_matrix=rf_cm, display_labels=None)


# Plot confusion matrix
rf_cm_disp.plot()

​

# Display plot
plt.title('Random forest - test set');
plt.show()

# Create a classification report

# Create classification report for random forest model

target_labels = ["Low_Traffic","High_Traffic"]

print(classification_report(y_test, y_pred, target_names=target_labels))
              precision    recall  f1-score   support

 Low_Traffic       0.68      0.47      0.56       106
High_Traffic       0.71      0.86      0.78       163

    accuracy                           0.71       269
   macro avg       0.70      0.67      0.67       269
weighted avg       0.70      0.71      0.69       269


from xgboost import plot_importance

importances = rf_cv.best_estimator_.feature_importances_
rf_importances = pd.Series(importances, index=X_test.columns)

fig, ax = plt.subplots()

rf_importances.plot.bar(ax=ax)
ax.set_title('Feature importances')
ax.set_ylabel('Mean decrease in impurity')
fig.tight_layout()


# Create a DataFrame to store the importances
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})

​

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display the table
print(feature_importance_df)
                   Feature  Importance
3                  protein    0.251572
13      category_Vegetable    0.128038
0                 calories    0.110246
1             carbohydrate    0.095384
2                    sugar    0.087229
12         category_Potato    0.074625
5       category_Breakfast    0.072198
6         category_Chicken    0.062231
11           category_Pork    0.037513
4                 servings    0.029315
10  category_One Dish Meal    0.026345
9            category_Meat    0.012143
8    category_Lunch/Snacks    0.009736
7         category_Dessert    0.003424
Feature Importance
Protein Power:
The most influential factor is "protein," with an importance score of around 0.252. This means that recipes packed with protein tend to catch people's attention the most.

Nutrition Matters:
"calories," "carbohydrate," and "sugar" follow as important contributors. It seems that people are drawn to recipes that have favorable nutritional aspects.

Categories that Click:
Certain recipe categories stand out, like "Vegetable," "Potato," and "Breakfast." These categories have a significant impact on drawing traffic, suggesting that recipes falling under these categories are especially appealing.

The Protein Effect:
Again, "protein" emerges as a top influencer, indicating that this nutritional aspect is highly compelling for users.

Varied Preferences:
The model considers a wide range of categories, from "Chicken" and "Meat" to "One Dish Meal" and "Lunch/Snacks." This tells us that recipes spanning various categories can catch people's interest.

Desserts' Place:
Surprisingly, "Dessert" comes up with the lowest importance score. This suggests that while desserts are enjoyed, they might not be the primary driver for recipe popularity.

In short, our Random Forest model shows us that nutritional content, recipe categories, and diversity all have a role in predicting high-traffic recipes. Protein-rich recipes, those with balanced nutritional profiles, and categories like vegetables and breakfast are more likely to capture attention. This information can guide us in selecting recipes that have a higher chance of becoming popular and engaging for our users.

Model evaluation:
Based on the provided classification reports for both the Random Forest and XGBoost models on the validation set, and the Random Forest model's performance on the testing set, let's draw a conclusion and answer the questions outlined in the email you sent:

Conclusion:

Validation Set Performance:

Both the Random Forest and XGBoost models show relatively similar performance on the validation set.
For predicting high traffic recipes, Random Forest has a slightly higher recall and f1-score, indicating it performs slightly better at identifying high traffic recipes.
Testing Set Performance (Random Forest):

The Random Forest model performs well on the testing set, achieving a reasonable balance between precision and recall for both low and high traffic recipes.
It achieves a higher recall for high traffic recipes (0.86) compared to low traffic recipes (0.47).
Can your team predict which recipes will lead to high traffic?

Both the Random Forest and XGBoost models exhibit the ability to predict high traffic recipes, with the Random Forest model performing slightly better in terms of recall on the validation set.

Can your team correctly predict high traffic recipes 80% of the time?
The Random Forest model achieves a recall of 0.86 for high traffic recipes on the testing set, which indicates that it can indeed correctly predict high traffic recipes with a rate exceeding 80%.

How Should the Business Keep Track of Their Goals?

To keep a close eye on what Tasty Bytes wants to achieve, they need to watch certain important numbers that match their business goals. Given their story, their main focus seems to be providing folks with healthy meal plans via a subscription service. Here's how they can do that:

1. Check How Many People Stick Around:
They should keep an eye on how many people continue their subscription every month. This will help them know if their meal plans are making customers happy.

2. See How People Are Using Their Plans:
Watching things like how often people use their recipes, how long they spend on their website or app, and how they interact with their meal plans can show if customers are getting value and enjoying the experience.

3.Make Sure Ingredients Are Delivered Right:
If they're delivering ingredients, it's important to make sure they're sending the right stuff to the right people in good shape.

4.Listen to What Customers Are Saying:
Reading customer feedback and reviews regularly will give them a sense of what people like and don't like. They could even use technology to figure out if most of the feedback is positive or not.

5.See Who's Signing Up:
If they have different subscription levels, they should keep track of how many people switch from being free users to paying subscribers. This shows how much people like their premium plans.

6.Watch How Their Website/App Is Doing:
They can use tools to see how many people are visiting their website or app, where they're clicking, and what sections they're interested in. This can help them figure out what's working and what could be better.

Estimating How Well They're Doing Right Now:
Category Distribution:
Analyzing the distribution of recipes across different categories (such as Pork, Potato, Breakfast, etc.) can provide insights into which types of recipes are more frequently chosen by users and the business can use these ingredients more frequently to attract subscribers. Besides, this can help in curating a diverse and appealing recipe collection which can be obtained from the current dataset which indicated below.

image-2

Beverages:
It looks like only 5 out of 92 beverage recipes are really popular (high traffic), while the rest aren't getting that much attention.

Breakfast:
Among the breakfast recipes, a good 33 out of 106 are catching people's interest, which shows that breakfast recipes are quite popular.

Chicken:
When it comes to chicken recipes, a whopping 69 out of 163 are getting lots of traffic, so people seem to love their chicken dishes.

Dessert:
Almost half of the dessert recipes (48 out of 77) are grabbing people's attention, which is a pretty sweet deal.

Lunch/Snacks:
For lunch and snack recipes, 52 out of 82 are in the high traffic zone, meaning they're quite popular.

Meat:
People are definitely into meat recipes – 56 out of 92 are high traffic, making them a hit.

One Dish Meal:
One-dish meals are a hit too, with 51 out of 118 recipes generating high traffic.

Pork:
Pork recipes are getting some love too, with 66 out of 73 in the high traffic category.

Potato:
The majority of potato recipes (78 out of 83) are high traffic, making them quite popular choices.

Vegetable:
Similar to potatoes, almost all vegetable recipes (77 out of 78) are high traffic, showing that veggies are a favorite.

In a nutshell, categories like "Chicken," "Dessert," "Meat," "One Dish Meal," "Pork," "Potato," and "Vegetable" have recipes that people really like, as they're getting a lot of attention. Categories like "Beverages" and "Lunch/Snacks" have fewer recipes that are super popular. "Breakfast" falls in the middle with a mix of both popular and not-so-popular recipes. This gives Tasty Bytes some insights on where to focus more attention and perhaps tweak their offerings to match what users are enjoying.

Summary and Recommendations:
Tasty Bytes should make sure to have a clear plan for keeping track of these numbers. They can use tools and technology to help. They should keep checking these numbers regularly, compare them to their goals, and use the information to make things even better. It's a good idea to talk about what they learn from these numbers with their team to make smart decisions as they go along.

Based on the performance of the models and the goal of correctly predicting high traffic recipes, it is recommended to deploy the Random Forest model for predicting recipe popularity. The Random Forest model's better recall on the validation set and its performance on the testing set suggest that it is more capable of identifying high traffic recipes accurately.

For the future:

1.Continuously monitor and validate the deployed model's performance on real-world data.

2.Collect feedback from users and stakeholders to further fine-tune and improve the model.

3.Regularly update and retrain the model to adapt to changing user preferences and trends.
